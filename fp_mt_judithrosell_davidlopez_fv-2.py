# -*- coding: utf-8 -*-
"""FP_MT_JudithRosell_DavidLopez_FV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RhzdDnoF3LpyLa3Ptzzj8n_Pgojp7RxV

# Building a MT model with backtranslation
"""

# First of all, we have downloaded from the OPUS parallel corpora database:

# - the ca-en WikiMatrix parallel corpus –> the ca file will be used as monolingual training set
# - the ca-en OpenSubtitles parallel corpus
# - the ca-es OpenSubtitles parallel corpus

# https://opus.nlpl.eu/OpenSubtitles.php

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocess the text files"""

# This is just to check the length of each file

with open('/content/drive/MyDrive/MT2/ca-en/init.train.mono.ca', 'r') as f_in:
  content = f_in.read()
  print(len(content))

with open('/content/drive/MyDrive/MT2/ca-en/init.ca-en.ca', 'r') as f_in_ca:
  content_ca = f_in_ca.read()
  print(len(content_ca))

import re

# Preprocess train.mono.ca
# Open the input file
with open('/content/drive/MyDrive/MT2/ca-en/init.train.mono.ca', 'r') as f_in:
    # Open the output file
    with open('/content/drive/MyDrive/MT2/ca-en/train.mono.ca', 'w') as f_out:
        # Loop over each line in the input file
        for line in f_in:
            # Remove special characters
            line = line.lower()
            line = re.sub('[«»"]', '', line)
            # Check if the line contains "(en anglès)"
            if '(en anglès)' not in line:
                # Write the line to the output file
                f_out.write(line)


# Preprocess parallel corpus
# Open the input file
with open('/content/drive/MyDrive/MT2/ca-en/init.ca-en.ca', 'r') as f_in_ca:
    # Open the output file
    with open('/content/drive/MyDrive/MT2/ca-en/ca-en.ca', 'w') as f_out_ca:
        # Loop over each line in the input file
        for line in f_in_ca:
            # Remove special characters
            line = line.lower()
            line = re.sub('[«»"]', '', line)
            # Write the line to the output file
            f_out_ca.write(line)


# Open the input file
with open('/content/drive/MyDrive/MT2/ca-en/init.ca-en.en', 'r') as f_in_en:
    # Open the output file
    with open('/content/drive/MyDrive/MT2/ca-en/ca-en.en', 'w') as f_out_en:
        # Loop over each line in the input file
        for line in f_in_en:
            # Remove special characters
            line = line.lower()
            line = re.sub('[«»"]', '', line)
            # Write the line to the output file
            f_out_en.write(line)

"""## Split the ca-en OpenSubtitles parallel corpus into train, dev and test sets"""

# We need to split the ca-en OpenSubtitles parallel corpus into train, dev and test sets

import os
import random

# Define the path to the files
path_ca = '/content/drive/MyDrive/MT2/ca-en/ca-en.ca'
path_en = '/content/drive/MyDrive/MT2/ca-en/ca-en.en'

# Define the percentages for the train, dev, and test sets
train_percent = 0.7
dev_percent = 0.15
test_percent = 0.15

# Define the output directory
output_dir = '/content/drive/MyDrive/MT2/ca-en'

# Create the output directory if it does not exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    
# Read the files
with open(path_ca, 'r', encoding='utf-8') as f:
    lines_ca = f.readlines()
with open(path_en, 'r', encoding='utf-8') as f:
    lines_en = f.readlines()

# Compute the sizes of the train, dev, and test sets
size = len(lines_ca)
train_size = int(size * train_percent)
dev_size = int(size * dev_percent)
test_size = size - train_size - dev_size

# Shuffle the lines
random.seed(42)
indices = list(range(size))
random.shuffle(indices)

# Split the lines into train, dev, and test sets
train_indices = indices[:train_size]
dev_indices = indices[train_size:train_size+dev_size]
test_indices = indices[train_size+dev_size:]

train_ca = [lines_ca[i] for i in train_indices]
train_en = [lines_en[i] for i in train_indices]
dev_ca = [lines_ca[i] for i in dev_indices]
dev_en = [lines_en[i] for i in dev_indices]
test_ca = [lines_ca[i] for i in test_indices]
test_en = [lines_en[i] for i in test_indices]

# Write the output files
with open(os.path.join(output_dir, 'train.ca-en.ca'), 'w', encoding='utf-8') as f:
    f.writelines(train_ca)
with open(os.path.join(output_dir, 'train.ca-en.en'), 'w', encoding='utf-8') as f:
    f.writelines(train_en)
with open(os.path.join(output_dir, 'dev.ca-en.ca'), 'w', encoding='utf-8') as f:
    f.writelines(dev_ca)
with open(os.path.join(output_dir, 'dev.ca-en.en'), 'w', encoding='utf-8') as f:
    f.writelines(dev_en)
with open(os.path.join(output_dir, 'test.ca-en.ca'), 'w', encoding='utf-8') as f:
    f.writelines(test_ca)
with open(os.path.join(output_dir, 'test.ca-en.en'), 'w', encoding='utf-8') as f:
    f.writelines(test_en)

"""## Split the ca-es OpenSubtitles parallel corpus into train, dev and test set"""

# We need to split the ca-en OpenSubtitles parallel corpus into train, dev and test sets

import os
import random

# Define the path to the files
path_ca = '/content/drive/MyDrive/MT2/ca-en/ca-es.ca'
path_en = '/content/drive/MyDrive/MT2/ca-en/ca-es.es'

# Define the percentages for the train, dev, and test sets
train_percent = 0.7
dev_percent = 0.15
test_percent = 0.15

# Define the output directory
output_dir = '/content/drive/MyDrive/MT2/ca-en'

# Create the output directory if it does not exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    
# Read the files
with open(path_ca, 'r', encoding='utf-8') as f:
    lines_ca = f.readlines()
with open(path_en, 'r', encoding='utf-8') as f:
    lines_en = f.readlines()

# Compute the sizes of the train, dev, and test sets
size = len(lines_ca)
train_size = int(size * train_percent)
dev_size = int(size * dev_percent)
test_size = size - train_size - dev_size

# Shuffle the lines
random.seed(42)
indices = list(range(size))
random.shuffle(indices)

# Split the lines into train, dev, and test sets
train_indices = indices[:train_size]
dev_indices = indices[train_size:train_size+dev_size]
test_indices = indices[train_size+dev_size:]

train_ca = [lines_ca[i] for i in train_indices]
train_es = [lines_en[i] for i in train_indices]
dev_ca = [lines_ca[i] for i in dev_indices]
dev_es = [lines_en[i] for i in dev_indices]
test_ca = [lines_ca[i] for i in test_indices]
test_es = [lines_en[i] for i in test_indices]

# Write the output files
with open(os.path.join(output_dir, 'train.ca-es.ca'), 'w', encoding='utf-8') as f:
    f.writelines(train_ca)
with open(os.path.join(output_dir, 'train.ca-es.es'), 'w', encoding='utf-8') as f:
    f.writelines(train_es)
with open(os.path.join(output_dir, 'dev.ca-es.ca'), 'w', encoding='utf-8') as f:
    f.writelines(dev_ca)
with open(os.path.join(output_dir, 'dev.ca-es.es'), 'w', encoding='utf-8') as f:
    f.writelines(dev_es)
with open(os.path.join(output_dir, 'test.ca-es.ca'), 'w', encoding='utf-8') as f:
    f.writelines(test_ca)
with open(os.path.join(output_dir, 'test.ca-es.es'), 'w', encoding='utf-8') as f:
    f.writelines(test_es)

"""## Set source and target languages"""

# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:
# These will also become the suffix's of all vocab and corpus files used throughout
import os
from os import path

source_language = "en"
target_language = "ca" 
lc = False  # If True, lowercase the data.
seed = 42  # Random seed for shuffling.
tag = "baseline" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted

os.environ["src"] = source_language # Sets them in bash as well, since we often use bash scripts
os.environ["tgt"] = target_language
os.environ["tag"] = tag

# This will save it to a folder in our gdrive instead!
!mkdir -p "/content/drive/MyDrive/MT2"
os.environ["gdrive_path"] = "/content/drive/MyDrive/MT2"

!echo "$gdrive_path"
!mkdir -p "/content/drive/MyDrive/MT2"

"""

---


## Installation of JoeyNMT

JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  """

! pip install -e git+https://github.com/joeynmt/joeynmt.git@1.5#egg=joeynmt
! rm -fr joeynmt
! git clone https://github.com/joeynmt/joeynmt.git --branch 1.5 --single-branch
! pip install torch==1.10.1+cu102 torchtext==0.11.1 -f https://download.pytorch.org/whl/torch_stable.html
! pip install setuptools==59.5.0

"""## Prepare the training data
Combine low-resourced langauge (ca) and high-resourced language (es) to get the best training data possible
"""

# copy data from your own Drive 
from os import path

# Replace "source_language" and "target_language" with the language codes of your choice
source_language = "en"
target_language = "ca"

# Set the path to the data files on your Google Drive
data_drive_path = "/content/drive/MyDrive/MT2/ca-en"

# Set the destination path for the data files in the Colab environment
data_path = path.join("joeynmt", "data", source_language + target_language)
os.environ["data_path"] = data_path

# Create the destination directory if it does not exist
!mkdir -p $data_path

# Copy the data files from your Google Drive to the destination directory
!cp -r "$data_drive_path"/* $data_path/

'''# copy data from your onw Drive 
from os import path

data_path = path.join("joeynmt", "data", source_language + target_language)
os.environ["data_path"] = data_path

!mkdir -p $data_path
!cp /content/drive/MyDrive/MT/ca-en/* $data_path'''

# back-translation
!pip install sentencepiece
!pip install transformers

"""# Generate bpe model"""

!cat $data_path/train.ca-en.en $data_path/train.ca-en.ca | subword-nmt learn-bpe -s 20000 -o bpe.codes

!cat $data_path/train.ca-es.es $data_path/train.ca-es.ca | subword-nmt learn-bpe -s 20000 -o esbpe.codes

"""#Backtranslation: translate CA –> ES"""

# load target language monolingual data into src_text

from transformers import MarianTokenizer, MarianMTModel
from transformers import pipeline
from typing import List

#load the model from https://huggingface.co/Helsinki-NLP
model_name = 'Helsinki-NLP/opus-mt-ca-es'
model = MarianMTModel.from_pretrained(model_name)
tokenizer = MarianTokenizer.from_pretrained(model_name)

# Translate data: 
monolingual_ca = open('/content/drive/MyDrive/MT2/ca-en/train.mono.ca', 'r').readlines()
tgt_file = open('/content/drive/MyDrive/MT2/ca-en/train.bcktr.es', 'w', )
for line in monolingual_ca[:10000]: # Delete '[:200]' to translate everything –> Adapt size to computing power
  input = tokenizer(line, return_tensors="pt", padding=True)
  translated = model.generate(**input)
  tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
  #print(tgt_text)
  #print(type(tgt_text))
  tgt_file.write(tgt_text[0] + '\n')
tgt_file.close()

# apply BPE model to the split of the monolingual EN data

monolingual_ca = open('/content/drive/MyDrive/MT2/ca-en/train.mono.ca', 'r').readlines()

with open("split.train.mono.ca", "w") as src_file:
  for text in monolingual_ca[:249]:
    src_file.write(text)
!subword-nmt apply-bpe -c '/content/bpe.codes' < split.train.mono.ca > split.train.mono.bpe.ca

# apply BPE model to backtranslated data
!subword-nmt apply-bpe -c '/content/esbpe.codes' < '/content/joeynmt/data/enca/train.bcktr.es' > train.bcktr.bpe.es

!cat $data_path/train.bcktr.bpe.es

"""#Backtranslation: translate ES –> EN"""

# Load model
model_name = 'Helsinki-NLP/opus-mt-es-en'
model = MarianMTModel.from_pretrained(model_name)
tokenizer = MarianTokenizer.from_pretrained(model_name)

!pip install sacremoses

src_text = open('/content/drive/MyDrive/MT2/ca-en/train.bcktr.es', 'r').readlines()
tgt_file = open('/content/drive/MyDrive/MT2/ca-en/train.bcktr.en', 'w')
for line in src_text:
  input = tokenizer(line, return_tensors="pt", padding=True)
  translated = model.generate(**input)
  tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
  tgt_file.write(tgt_text[0] + '\n')
tgt_file.close()

'''translated = model.generate(**tokenizer(src_text, return_tensors="pt", padding=True))
tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
with open('/content/drive/MyDrive/MT/ca-en/train.bcktr.en', 'w') as tgt_file:
  for text in tgt_text:
    tgt_file.write(text + '\n')
tgt_file.close()'''

"""## Apply BPE model"""

# apply BPE model to ca-en training data
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/train.ca-en.en > $data_path/train.ca-en.bpe.en
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/train.ca-en.ca > $data_path/train.ca-en.bpe.ca

# apply BPE model to ca-es train data
!subword-nmt apply-bpe -c '/content/esbpe.codes' < $data_path/train.ca-es.es > $data_path/train.ca-es.bpe.es
!subword-nmt apply-bpe -c '/content/esbpe.codes' < $data_path/train.ca-es.ca > $data_path/train.ca-es.bpe.ca

# apply BPE model to ca-en validation data
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/dev.ca-en.en > $data_path/dev.ca-en.bpe.en
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/dev.ca-en.ca > $data_path/dev.ca-en.bpe.ca

# apply BPE model to ca-en test data
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/test.ca-en.en > $data_path/test.ca-en.bpe.en
!subword-nmt apply-bpe -c '/content/bpe.codes' < $data_path/test.ca-en.ca > $data_path/test.ca-en.bpe.ca

# apply BPE model to backtranslated en data
!subword-nmt apply-bpe -c '/content/bpe.codes' < '/content/joeynmt/data/enca/train.bcktr.en' > train.bcktr.bpe.en

# Combine the partial corpora to create final resources
!cat $data_path/train.ca-en.bpe.en $data_path/train.ca-es.bpe.es $data_path/train.bcktr.bpe.es $data_path/train.bcktr.bpe.en > $data_path/train.bpe.en
!cat $data_path/train.ca-en.bpe.ca $data_path/train.ca-es.bpe.ca $data_path/split.train.mono.bpe.ca $data_path/split.train.mono.bpe.ca > $data_path/train.bpe.ca

# Create that vocab using build_vocab
! sudo chmod 777 joeynmt/scripts/build_vocab.py
! joeynmt/scripts/build_vocab.py $data_path/train.bpe.en $data_path/train.bpe.ca --output_path $data_path/vocab.txt

"""

---



---



---



---



---

"""