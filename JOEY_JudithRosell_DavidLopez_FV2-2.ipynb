{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Low Resourced Neural Machine Translation (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change Python's version to solve compatibility with PyTorch and JoeyNMT.We will select the option 2."
      ],
      "metadata": {
        "id": "zzEeW1Uz7XOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4iVzAZLsV6G",
        "outputId": "cb3c0415-00a7-4f9f-d466-7e08590487bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.10   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.8    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Python 3.8.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 2s (1,254 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ],
      "source": [
        "#**Add python version you wish** to list\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "\n",
        "# Choose one of the given alternatives:\n",
        "!sudo update-alternatives --config python3\n",
        "\n",
        "# This one used to work but now NOT(for me)!\n",
        "# !sudo update-alternatives --config python\n",
        "\n",
        "# Check the result\n",
        "!python3 --version\n",
        "\n",
        "# Attention: Install pip (... needed!)\n",
        "!sudo apt install python3-pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb879a2-64bf-43a8-bb7f-72439340f732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce sentences in files\n",
        "\n",
        "with open('/content/drive/MyDrive/MT2/ca-en/train.bpe-2.ca', 'r') as f_in:\n",
        "    with open('/content/drive/MyDrive/MT2/ca-en/train.bpeS.ca', 'w') as f_out:\n",
        "      lines = f_in.readlines()[:500]\n",
        "      f_out.writelines(lines)\n",
        "\n",
        "with open('/content/drive/MyDrive/MT2/ca-en/train.bpe-2.en', 'r') as f_in:\n",
        "    with open('/content/drive/MyDrive/MT2/ca-en/train.bpeS.en', 'w') as f_out:\n",
        "      lines = f_in.readlines()[:500]\n",
        "      f_out.writelines(lines)\n",
        "\n",
        "with open('/content/drive/MyDrive/MT2/ca-en/dev.ca-en.bpe.ca', 'r') as f_in:\n",
        "    with open('/content/drive/MyDrive/MT2/ca-en/dev.ca-en.bpeS.ca', 'w') as f_out:\n",
        "      lines = f_in.readlines()[:500]\n",
        "      f_out.writelines(lines)\n",
        "\n",
        "with open('/content/drive/MyDrive/MT2/ca-en/dev.ca-en.bpe.en', 'r') as f_in:\n",
        "    with open('/content/drive/MyDrive/MT2/ca-en/dev.ca-en.bpeS.en', 'w') as f_out:\n",
        "      lines = f_in.readlines()[:500]\n",
        "      f_out.writelines(lines)"
      ],
      "metadata": {
        "id": "nvwsZhuqXhyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set source and target language"
      ],
      "metadata": {
        "id": "7I-M0OQ43wO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn3tgQLzUxwn"
      },
      "outputs": [],
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "source_language = \"en\"\n",
        "target_language = \"ca\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/MyDrive/MT2\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/MyDrive/MT2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9a79a40-d27f-4470-9052-4cfc99954360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining joeynmt from git+https://github.com/joeynmt/joeynmt.git@1.5#egg=joeynmt\n",
            "  Cloning https://github.com/joeynmt/joeynmt.git (to revision 1.5) to ./src/joeynmt\n",
            "  Running command git clone -q https://github.com/joeynmt/joeynmt.git /content/src/joeynmt\n",
            "  Running command git checkout -q 092c504cb3d7b25b91cc37af4fbfe55af4faf64f\n",
            "Collecting future\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[K     |████████████████████████████████| 840 kB 35.3 MB/s \n",
            "\u001b[?25hCollecting matplotlib\n",
            "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 86.4 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.19.5\n",
            "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 89.1 MB/s \n",
            "\u001b[?25hCollecting pillow\n",
            "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.2 MB/s \n",
            "\u001b[?25hCollecting pylint>=2.9.6\n",
            "  Downloading pylint-2.17.4-py3-none-any.whl (536 kB)\n",
            "\u001b[K     |████████████████████████████████| 536 kB 82.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[K     |████████████████████████████████| 701 kB 91.1 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=2.0.0\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting seaborn\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[K     |████████████████████████████████| 293 kB 113.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from joeynmt) (45.2.0)\n",
            "Collecting six>=1.12\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting tensorboard>=1.15\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 84.3 MB/s \n",
            "\u001b[?25hCollecting torch>=1.9.0\n",
            "  Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 619.9 MB 26 kB/s \n",
            "\u001b[?25hCollecting torchtext>=0.10.0\n",
            "  Downloading torchtext-0.15.1-cp38-cp38-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 72.9 MB/s \n",
            "\u001b[?25hCollecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Collecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting python-dateutil>=2.7\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 80.6 MB/s \n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 74.4 MB/s \n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.39.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 77.8 MB/s \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyparsing>=2.3.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[K     |████████████████████████████████| 300 kB 110.5 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.8,>=0.6\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting typing-extensions>=3.10.0; python_version < \"3.10\"\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting tomli>=1.1.0; python_version < \"3.11\"\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting astroid<=2.17.0-dev0,>=2.15.4\n",
            "  Downloading astroid-2.15.4-py3-none-any.whl (278 kB)\n",
            "\u001b[K     |████████████████████████████████| 278 kB 90.5 MB/s \n",
            "\u001b[?25hCollecting dill>=0.2; python_version < \"3.11\"\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 100.8 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-3.5.0-py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit>=0.10.1\n",
            "  Downloading tomlkit-0.11.8-py3-none-any.whl (35 kB)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 11.7 MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 80.3 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting lxml\n",
            "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 61.4 MB/s \n",
            "\u001b[?25hCollecting tabulate>=0.8.9\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting pandas>=0.25\n",
            "  Downloading pandas-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3 MB 89.3 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Collecting protobuf>=3.19.6\n",
            "  Downloading protobuf-4.22.4-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[K     |████████████████████████████████| 302 kB 114.2 MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 89.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=1.15->joeynmt) (0.34.2)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.48.2\n",
            "  Downloading grpcio-1.54.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1 MB 88.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.3.3-py3-none-any.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 90.4 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 80.0 MB/s \n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 168.4 MB 60 kB/s \n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 102.6 MB 2.7 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 82.0 MB/s \n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 173.2 MB 26 kB/s \n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.6 MB 293 kB/s \n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 68.8 MB/s \n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 79.5 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 9.5 kB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 177.1 MB 76 kB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 28 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 88 kB/s \n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 97.9 MB/s \n",
            "\u001b[?25hCollecting triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 66.1 MB/s \n",
            "\u001b[?25hCollecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 79.8 MB/s \n",
            "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\"\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.9.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[K     |████████████████████████████████| 502 kB 78.8 MB/s \n",
            "\u001b[?25hCollecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[K     |████████████████████████████████| 341 kB 88.6 MB/s \n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 87.5 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 1.2 kB/s \n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 94.5 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 97.5 MB/s \n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[K     |████████████████████████████████| 536 kB 77.6 MB/s \n",
            "\u001b[?25hCollecting cmake\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.3.tar.gz (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 82.5 MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 93.2 MB/s \n",
            "\u001b[?25hCollecting pyasn1>=0.1.3\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future, wrapt, lit\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=419bddb88204fc5676bfb81254db550481c4865bf6e86099fa0e73f93676c78f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp38-cp38-linux_x86_64.whl size=78282 sha256=c2b879a28b5784fce1ed1e85a155fbe706ba2b71cd2a5a759ae68a05bcf0070e\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/83/58efda72eb47567053254faec24fe048dced315a0c3f11e8f8\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.3-py3-none-any.whl size=88191 sha256=c64dfabac2a00493513f05ff58b888b979889fd8b1f94efdff0eff9caf7b97d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/20/c2/b32a74e477abe541734225f8d5ed8b9e284d1f8580f6828445\n",
            "Successfully built future wrapt lit\n",
            "Installing collected packages: future, numpy, zipp, importlib-resources, six, python-dateutil, kiwisolver, fonttools, cycler, pillow, pyparsing, contourpy, packaging, matplotlib, mccabe, typing-extensions, tomli, wrapt, lazy-object-proxy, astroid, dill, platformdirs, tomlkit, isort, pylint, pyyaml, regex, colorama, portalocker, lxml, tabulate, sacrebleu, pytz, tzdata, pandas, seaborn, tqdm, mock, subword-nmt, protobuf, urllib3, certifi, idna, charset-normalizer, requests, tensorboard-data-server, importlib-metadata, markdown, grpcio, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, MarkupSafe, werkzeug, absl-py, tensorboard, nvidia-cufft-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cuda-runtime-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, mpmath, sympy, networkx, nvidia-cudnn-cu11, filelock, nvidia-nccl-cu11, nvidia-cuda-nvrtc-cu11, nvidia-nvtx-cu11, jinja2, cmake, lit, triton, nvidia-cuda-cupti-cu11, torch, torchdata, torchtext, joeynmt\n",
            "  Running setup.py develop for joeynmt\n",
            "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astroid-2.15.4 cachetools-5.3.0 certifi-2023.5.7 charset-normalizer-3.1.0 cmake-3.26.3 colorama-0.4.6 contourpy-1.0.7 cycler-0.11.0 dill-0.3.6 filelock-3.12.0 fonttools-4.39.3 future-0.18.3 google-auth-2.17.3 google-auth-oauthlib-1.0.0 grpcio-1.54.0 idna-3.4 importlib-metadata-6.6.0 importlib-resources-5.12.0 isort-5.12.0 jinja2-3.1.2 joeynmt kiwisolver-1.4.4 lazy-object-proxy-1.9.0 lit-16.0.3 lxml-4.9.2 markdown-3.4.3 matplotlib-3.7.1 mccabe-0.7.0 mock-5.0.2 mpmath-1.3.0 networkx-3.1 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 oauthlib-3.2.2 packaging-23.1 pandas-2.0.1 pillow-9.5.0 platformdirs-3.5.0 portalocker-2.7.0 protobuf-4.22.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 pylint-2.17.4 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 regex-2023.5.5 requests-2.30.0 requests-oauthlib-1.3.1 rsa-4.9 sacrebleu-2.3.1 seaborn-0.12.2 six-1.16.0 subword-nmt-0.3.8 sympy-1.11.1 tabulate-0.9.0 tensorboard-2.13.0 tensorboard-data-server-0.7.0 tomli-2.0.1 tomlkit-0.11.8 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 tqdm-4.65.0 triton-2.0.0 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.2 werkzeug-2.3.3 wrapt-1.11.1 zipp-3.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "dateutil",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 3292, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 3292 (delta 10), reused 9 (delta 9), pack-reused 3278\u001b[K\n",
            "Receiving objects: 100% (3292/3292), 8.10 MiB | 9.96 MiB/s, done.\n",
            "Resolving deltas: 100% (2280/2280), done.\n",
            "Note: switching to '092c504cb3d7b25b91cc37af4fbfe55af4faf64f'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.1+cu102\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torch-1.10.1%2Bcu102-cp38-cp38-linux_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 8.2 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.11.1\n",
            "  Downloading torchtext-0.11.1-cp38-cp38-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 23.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu102) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.1) (2.30.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.1) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.1) (4.65.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.1) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.1) (2.0.2)\n",
            "\u001b[31mERROR: torchdata 0.6.0 has requirement torch==2.0.0, but you'll have torch 1.10.1+cu102 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "Successfully installed torch-1.10.1+cu102 torchtext-0.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 29.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 45.2.0\n",
            "    Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'setuptools'. No files were found to uninstall.\n",
            "Successfully installed setuptools-59.5.0\n"
          ]
        }
      ],
      "source": [
        "# This takes about 7-8 minutes\n",
        "! pip install -e git+https://github.com/joeynmt/joeynmt.git@1.5#egg=joeynmt\n",
        "! rm -fr joeynmt\n",
        "! git clone https://github.com/joeynmt/joeynmt.git --branch 1.5 --single-branch\n",
        "! pip install torch==1.10.1+cu102 torchtext==0.11.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install setuptools==59.5.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2p8qQpPqoM-"
      },
      "source": [
        "# Prepare the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FopZpiJHDO_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffddc7d0-cb11-41e7-bf30-336a49f4ff78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joeynmt/data\n"
          ]
        }
      ],
      "source": [
        "# copy data from your own Drive \n",
        "from os import path\n",
        "\n",
        "data_path = path.join(\"joeynmt\", \"data\")\n",
        "print(data_path)\n",
        "os.environ[\"data_path\"] = data_path\n",
        "\n",
        "!mkdir -p $data_path\n",
        "!cp /content/drive/MyDrive/MT2/ca-en/* $data_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIs1lY2hxMsl"
      },
      "outputs": [],
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"/content/joeynmt/data/train.bpeS\"\n",
        "    dev:   \"/content/joeynmt/data/dev.ca-en.bpeS\"\n",
        "    test:  \"/content/joeynmt/data/dev.ca-en.bpeS\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"/content/joeynmt/data/vocab-2.txt\"\n",
        "    trg_vocab: \"/content/joeynmt/data/vocab-2.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 500                   # TODO: Decrease for when playing around and checking of working. Around 50 is sufficient to check if its working at all\n",
        "    validation_freq: 100          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 50\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2641489-841c-46a5-be07-3b867f335ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-07 13:32:28,811 - INFO - root - Hello! This is Joey-NMT (version 1.5).\n",
            "2023-05-07 13:32:28,859 - INFO - joeynmt.data - Loading training data...\n",
            "2023-05-07 13:32:28,862 - INFO - joeynmt.data - Building vocabulary...\n",
            "2023-05-07 13:32:39,251 - INFO - joeynmt.data - Loading dev data...\n",
            "2023-05-07 13:32:39,257 - INFO - joeynmt.data - Loading test data...\n",
            "2023-05-07 13:32:39,260 - INFO - joeynmt.data - Data loaded.\n",
            "2023-05-07 13:32:39,260 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-05-07 13:32:39,612 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2023-05-07 13:32:39,616 - INFO - joeynmt.training - Total params: 19139584\n",
            "2023-05-07 13:32:39,618 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                           cfg.name : enca_transformer\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                       cfg.data.src : en\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                       cfg.data.trg : ca\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                     cfg.data.train : /content/joeynmt/data/train.bpeS\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/joeynmt/data/dev.ca-en.bpeS\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                      cfg.data.test : /content/joeynmt/data/dev.ca-en.bpeS\n",
            "2023-05-07 13:32:43,053 - INFO - joeynmt.helpers -                     cfg.data.level : bpe\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -                 cfg.data.lowercase : False\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -           cfg.data.max_sent_length : 100\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -                 cfg.data.src_vocab : /content/joeynmt/data/vocab-2.txt\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -                 cfg.data.trg_vocab : /content/joeynmt/data/vocab-2.txt\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -     cfg.testing.sacrebleu.tokenize : none\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -              cfg.training.patience : 5\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\n",
            "2023-05-07 13:32:43,054 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 3600\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -                cfg.training.epochs : 500\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 100\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 50\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/enca_transformer\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -       cfg.training.keep_last_ckpts : 3\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
            "2023-05-07 13:32:43,055 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2023-05-07 13:32:43,056 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 500,\n",
            "\tvalid 500,\n",
            "\ttest 500\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] - un@@ -@@ fuc@@ king@@ -@@ believ@@ able.\n",
            "\t[TRG] - és fotudament increïble.\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) a (6) de (7) - (8) la (9) i\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) a (6) de (7) - (8) la (9) i\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - Number of Src words (types): 31560\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.helpers - Number of Trg words (types): 31560\n",
            "2023-05-07 13:32:43,057 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=31560),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=31560))\n",
            "2023-05-07 13:32:43,067 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2023-05-07 13:32:43,067 - INFO - joeynmt.training - EPOCH 1\n",
            "2023-05-07 13:32:44,279 - INFO - joeynmt.training - Epoch   1: total training loss 52.68\n",
            "2023-05-07 13:32:44,279 - INFO - joeynmt.training - EPOCH 2\n",
            "2023-05-07 13:32:45,294 - INFO - joeynmt.training - Epoch   2: total training loss 41.45\n",
            "2023-05-07 13:32:45,294 - INFO - joeynmt.training - EPOCH 3\n",
            "2023-05-07 13:32:46,351 - INFO - joeynmt.training - Epoch   3: total training loss 39.14\n",
            "2023-05-07 13:32:46,351 - INFO - joeynmt.training - EPOCH 4\n",
            "2023-05-07 13:32:47,558 - INFO - joeynmt.training - Epoch   4: total training loss 43.89\n",
            "2023-05-07 13:32:47,558 - INFO - joeynmt.training - EPOCH 5\n",
            "2023-05-07 13:32:48,702 - INFO - joeynmt.training - Epoch   5: total training loss 34.41\n",
            "2023-05-07 13:32:48,702 - INFO - joeynmt.training - EPOCH 6\n",
            "2023-05-07 13:32:49,775 - INFO - joeynmt.training - Epoch   6: total training loss 32.72\n",
            "2023-05-07 13:32:49,776 - INFO - joeynmt.training - EPOCH 7\n",
            "2023-05-07 13:32:50,911 - INFO - joeynmt.training - Epoch   7: total training loss 31.28\n",
            "2023-05-07 13:32:50,912 - INFO - joeynmt.training - EPOCH 8\n",
            "2023-05-07 13:32:51,990 - INFO - joeynmt.training - Epoch   8: total training loss 30.21\n",
            "2023-05-07 13:32:51,991 - INFO - joeynmt.training - EPOCH 9\n",
            "2023-05-07 13:32:53,027 - INFO - joeynmt.training - Epoch   9: total training loss 29.37\n",
            "2023-05-07 13:32:53,028 - INFO - joeynmt.training - EPOCH 10\n",
            "2023-05-07 13:32:53,633 - INFO - joeynmt.training - Epoch  10, Step:       50, Batch Loss:     5.754338, Tokens per Sec:     4076, Lr: 0.000300\n",
            "2023-05-07 13:32:54,161 - INFO - joeynmt.training - Epoch  10: total training loss 34.69\n",
            "2023-05-07 13:32:54,161 - INFO - joeynmt.training - EPOCH 11\n",
            "2023-05-07 13:32:55,197 - INFO - joeynmt.training - Epoch  11: total training loss 28.35\n",
            "2023-05-07 13:32:55,197 - INFO - joeynmt.training - EPOCH 12\n",
            "2023-05-07 13:32:56,329 - INFO - joeynmt.training - Epoch  12: total training loss 28.09\n",
            "2023-05-07 13:32:56,329 - INFO - joeynmt.training - EPOCH 13\n",
            "2023-05-07 13:32:57,461 - INFO - joeynmt.training - Epoch  13: total training loss 33.93\n",
            "2023-05-07 13:32:57,461 - INFO - joeynmt.training - EPOCH 14\n",
            "2023-05-07 13:32:58,592 - INFO - joeynmt.training - Epoch  14: total training loss 27.89\n",
            "2023-05-07 13:32:58,592 - INFO - joeynmt.training - EPOCH 15\n",
            "2023-05-07 13:32:59,654 - INFO - joeynmt.training - Epoch  15: total training loss 33.69\n",
            "2023-05-07 13:32:59,655 - INFO - joeynmt.training - EPOCH 16\n",
            "2023-05-07 13:33:00,756 - INFO - joeynmt.training - Epoch  16: total training loss 27.84\n",
            "2023-05-07 13:33:00,756 - INFO - joeynmt.training - EPOCH 17\n",
            "2023-05-07 13:33:01,872 - INFO - joeynmt.training - Epoch  17: total training loss 27.80\n",
            "2023-05-07 13:33:01,873 - INFO - joeynmt.training - EPOCH 18\n",
            "2023-05-07 13:33:02,942 - INFO - joeynmt.training - Epoch  18: total training loss 27.73\n",
            "2023-05-07 13:33:02,943 - INFO - joeynmt.training - EPOCH 19\n",
            "2023-05-07 13:33:03,964 - INFO - joeynmt.training - Epoch  19, Step:      100, Batch Loss:     5.561778, Tokens per Sec:     4155, Lr: 0.000300\n",
            "2023-05-07 13:33:04,621 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tHypothesis: \n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tHypothesis: \n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:33:05,043 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - \tHypothesis: \n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - \tHypothesis: \n",
            "2023-05-07 13:33:05,044 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step      100: bleu:   0.00, loss: 29454.3418, ppl: 914.2781, duration: 1.0798s\n",
            "2023-05-07 13:33:05,045 - INFO - joeynmt.training - Epoch  19: total training loss 27.70\n",
            "2023-05-07 13:33:05,045 - INFO - joeynmt.training - EPOCH 20\n",
            "2023-05-07 13:33:06,154 - INFO - joeynmt.training - Epoch  20: total training loss 27.62\n",
            "2023-05-07 13:33:06,155 - INFO - joeynmt.training - EPOCH 21\n",
            "2023-05-07 13:33:07,269 - INFO - joeynmt.training - Epoch  21: total training loss 27.54\n",
            "2023-05-07 13:33:07,269 - INFO - joeynmt.training - EPOCH 22\n",
            "2023-05-07 13:33:08,387 - INFO - joeynmt.training - Epoch  22: total training loss 27.44\n",
            "2023-05-07 13:33:08,388 - INFO - joeynmt.training - EPOCH 23\n",
            "2023-05-07 13:33:09,538 - INFO - joeynmt.training - Epoch  23: total training loss 32.94\n",
            "2023-05-07 13:33:09,539 - INFO - joeynmt.training - EPOCH 24\n",
            "2023-05-07 13:33:10,592 - INFO - joeynmt.training - Epoch  24: total training loss 27.20\n",
            "2023-05-07 13:33:10,592 - INFO - joeynmt.training - EPOCH 25\n",
            "2023-05-07 13:33:11,651 - INFO - joeynmt.training - Epoch  25: total training loss 27.13\n",
            "2023-05-07 13:33:11,651 - INFO - joeynmt.training - EPOCH 26\n",
            "2023-05-07 13:33:12,671 - INFO - joeynmt.training - Epoch  26: total training loss 26.98\n",
            "2023-05-07 13:33:12,671 - INFO - joeynmt.training - EPOCH 27\n",
            "2023-05-07 13:33:13,825 - INFO - joeynmt.training - Epoch  27: total training loss 26.86\n",
            "2023-05-07 13:33:13,826 - INFO - joeynmt.training - EPOCH 28\n",
            "2023-05-07 13:33:14,893 - INFO - joeynmt.training - Epoch  28: total training loss 26.72\n",
            "2023-05-07 13:33:14,893 - INFO - joeynmt.training - EPOCH 29\n",
            "2023-05-07 13:33:15,761 - INFO - joeynmt.training - Epoch  29, Step:      150, Batch Loss:     5.330795, Tokens per Sec:     4482, Lr: 0.000300\n",
            "2023-05-07 13:33:15,857 - INFO - joeynmt.training - Epoch  29: total training loss 26.63\n",
            "2023-05-07 13:33:15,857 - INFO - joeynmt.training - EPOCH 30\n",
            "2023-05-07 13:33:17,069 - INFO - joeynmt.training - Epoch  30: total training loss 31.59\n",
            "2023-05-07 13:33:17,070 - INFO - joeynmt.training - EPOCH 31\n",
            "2023-05-07 13:33:18,070 - INFO - joeynmt.training - Epoch  31: total training loss 26.18\n",
            "2023-05-07 13:33:18,070 - INFO - joeynmt.training - EPOCH 32\n",
            "2023-05-07 13:33:19,210 - INFO - joeynmt.training - Epoch  32: total training loss 26.04\n",
            "2023-05-07 13:33:19,211 - INFO - joeynmt.training - EPOCH 33\n",
            "2023-05-07 13:33:20,310 - INFO - joeynmt.training - Epoch  33: total training loss 25.93\n",
            "2023-05-07 13:33:20,310 - INFO - joeynmt.training - EPOCH 34\n",
            "2023-05-07 13:33:21,298 - INFO - joeynmt.training - Epoch  34: total training loss 25.66\n",
            "2023-05-07 13:33:21,299 - INFO - joeynmt.training - EPOCH 35\n",
            "2023-05-07 13:33:22,303 - INFO - joeynmt.training - Epoch  35: total training loss 25.63\n",
            "2023-05-07 13:33:22,304 - INFO - joeynmt.training - EPOCH 36\n",
            "2023-05-07 13:33:23,436 - INFO - joeynmt.training - Epoch  36: total training loss 25.49\n",
            "2023-05-07 13:33:23,437 - INFO - joeynmt.training - EPOCH 37\n",
            "2023-05-07 13:33:24,522 - INFO - joeynmt.training - Epoch  37: total training loss 25.36\n",
            "2023-05-07 13:33:24,522 - INFO - joeynmt.training - EPOCH 38\n",
            "2023-05-07 13:33:25,562 - INFO - joeynmt.training - Epoch  38: total training loss 25.26\n",
            "2023-05-07 13:33:25,563 - INFO - joeynmt.training - EPOCH 39\n",
            "2023-05-07 13:33:26,217 - INFO - joeynmt.training - Epoch  39, Step:      200, Batch Loss:     4.924566, Tokens per Sec:     3851, Lr: 0.000300\n",
            "2023-05-07 13:33:54,568 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tHypothesis: - - - - - - - - -\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tHypothesis: no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:33:55,066 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - \tHypothesis: - - - - - - - -\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - \tHypothesis: - - - - - - -\n",
            "2023-05-07 13:33:55,067 - INFO - joeynmt.training - Validation result (greedy) at epoch  39, step      200: bleu:   0.01, loss: 28772.3516, ppl: 780.7589, duration: 28.8500s\n",
            "2023-05-07 13:33:55,522 - INFO - joeynmt.training - Epoch  39: total training loss 25.10\n",
            "2023-05-07 13:33:55,522 - INFO - joeynmt.training - EPOCH 40\n",
            "2023-05-07 13:33:56,545 - INFO - joeynmt.training - Epoch  40: total training loss 25.07\n",
            "2023-05-07 13:33:56,546 - INFO - joeynmt.training - EPOCH 41\n",
            "2023-05-07 13:33:57,578 - INFO - joeynmt.training - Epoch  41: total training loss 24.83\n",
            "2023-05-07 13:33:57,578 - INFO - joeynmt.training - EPOCH 42\n",
            "2023-05-07 13:33:58,641 - INFO - joeynmt.training - Epoch  42: total training loss 24.81\n",
            "2023-05-07 13:33:58,642 - INFO - joeynmt.training - EPOCH 43\n",
            "2023-05-07 13:33:59,760 - INFO - joeynmt.training - Epoch  43: total training loss 24.62\n",
            "2023-05-07 13:33:59,760 - INFO - joeynmt.training - EPOCH 44\n",
            "2023-05-07 13:34:00,944 - INFO - joeynmt.training - Epoch  44: total training loss 29.37\n",
            "2023-05-07 13:34:00,944 - INFO - joeynmt.training - EPOCH 45\n",
            "2023-05-07 13:34:02,217 - INFO - joeynmt.training - Epoch  45: total training loss 29.20\n",
            "2023-05-07 13:34:02,217 - INFO - joeynmt.training - EPOCH 46\n",
            "2023-05-07 13:34:03,303 - INFO - joeynmt.training - Epoch  46: total training loss 24.25\n",
            "2023-05-07 13:34:03,303 - INFO - joeynmt.training - EPOCH 47\n",
            "2023-05-07 13:34:04,423 - INFO - joeynmt.training - Epoch  47: total training loss 24.20\n",
            "2023-05-07 13:34:04,423 - INFO - joeynmt.training - EPOCH 48\n",
            "2023-05-07 13:34:05,576 - INFO - joeynmt.training - Epoch  48: total training loss 24.04\n",
            "2023-05-07 13:34:05,576 - INFO - joeynmt.training - EPOCH 49\n",
            "2023-05-07 13:34:05,800 - INFO - joeynmt.training - Epoch  49, Step:      250, Batch Loss:     4.720708, Tokens per Sec:     5350, Lr: 0.000300\n",
            "2023-05-07 13:34:06,643 - INFO - joeynmt.training - Epoch  49: total training loss 23.97\n",
            "2023-05-07 13:34:06,644 - INFO - joeynmt.training - EPOCH 50\n",
            "2023-05-07 13:34:07,842 - INFO - joeynmt.training - Epoch  50: total training loss 23.83\n",
            "2023-05-07 13:34:07,842 - INFO - joeynmt.training - EPOCH 51\n",
            "2023-05-07 13:34:08,984 - INFO - joeynmt.training - Epoch  51: total training loss 23.68\n",
            "2023-05-07 13:34:08,984 - INFO - joeynmt.training - EPOCH 52\n",
            "2023-05-07 13:34:10,116 - INFO - joeynmt.training - Epoch  52: total training loss 23.54\n",
            "2023-05-07 13:34:10,116 - INFO - joeynmt.training - EPOCH 53\n",
            "2023-05-07 13:34:11,202 - INFO - joeynmt.training - Epoch  53: total training loss 23.48\n",
            "2023-05-07 13:34:11,203 - INFO - joeynmt.training - EPOCH 54\n",
            "2023-05-07 13:34:12,418 - INFO - joeynmt.training - Epoch  54: total training loss 23.30\n",
            "2023-05-07 13:34:12,419 - INFO - joeynmt.training - EPOCH 55\n",
            "2023-05-07 13:34:13,557 - INFO - joeynmt.training - Epoch  55: total training loss 23.24\n",
            "2023-05-07 13:34:13,557 - INFO - joeynmt.training - EPOCH 56\n",
            "2023-05-07 13:34:14,747 - INFO - joeynmt.training - Epoch  56: total training loss 23.04\n",
            "2023-05-07 13:34:14,747 - INFO - joeynmt.training - EPOCH 57\n",
            "2023-05-07 13:34:15,949 - INFO - joeynmt.training - Epoch  57: total training loss 22.94\n",
            "2023-05-07 13:34:15,949 - INFO - joeynmt.training - EPOCH 58\n",
            "2023-05-07 13:34:17,072 - INFO - joeynmt.training - Epoch  58: total training loss 22.78\n",
            "2023-05-07 13:34:17,072 - INFO - joeynmt.training - EPOCH 59\n",
            "2023-05-07 13:34:17,311 - INFO - joeynmt.training - Epoch  59, Step:      300, Batch Loss:     4.452109, Tokens per Sec:     2919, Lr: 0.000300\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - \tHypothesis: - - - - - ho ho ho bé.\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:34:48,647 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tHypothesis: no no no no no no no va que va la la va la va la la la la la la la la va la va la va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va va\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tHypothesis: - - - - - - - - ho bé.\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - \tHypothesis: - - - no.\n",
            "2023-05-07 13:34:48,648 - INFO - joeynmt.training - Validation result (greedy) at epoch  59, step      300: bleu:   0.01, loss: 29206.2461, ppl: 863.2505, duration: 31.3364s\n",
            "2023-05-07 13:34:49,663 - INFO - joeynmt.training - Epoch  59: total training loss 27.14\n",
            "2023-05-07 13:34:49,663 - INFO - joeynmt.training - EPOCH 60\n",
            "2023-05-07 13:34:50,701 - INFO - joeynmt.training - Epoch  60: total training loss 22.44\n",
            "2023-05-07 13:34:50,701 - INFO - joeynmt.training - EPOCH 61\n",
            "2023-05-07 13:34:51,861 - INFO - joeynmt.training - Epoch  61: total training loss 22.43\n",
            "2023-05-07 13:34:51,862 - INFO - joeynmt.training - EPOCH 62\n",
            "2023-05-07 13:34:52,916 - INFO - joeynmt.training - Epoch  62: total training loss 22.27\n",
            "2023-05-07 13:34:52,916 - INFO - joeynmt.training - EPOCH 63\n",
            "2023-05-07 13:34:53,991 - INFO - joeynmt.training - Epoch  63: total training loss 22.55\n",
            "2023-05-07 13:34:53,991 - INFO - joeynmt.training - EPOCH 64\n",
            "2023-05-07 13:34:55,183 - INFO - joeynmt.training - Epoch  64: total training loss 26.64\n",
            "2023-05-07 13:34:55,184 - INFO - joeynmt.training - EPOCH 65\n",
            "2023-05-07 13:34:56,367 - INFO - joeynmt.training - Epoch  65: total training loss 22.00\n",
            "2023-05-07 13:34:56,368 - INFO - joeynmt.training - EPOCH 66\n",
            "2023-05-07 13:34:57,419 - INFO - joeynmt.training - Epoch  66: total training loss 21.85\n",
            "2023-05-07 13:34:57,420 - INFO - joeynmt.training - EPOCH 67\n",
            "2023-05-07 13:34:58,588 - INFO - joeynmt.training - Epoch  67: total training loss 21.80\n",
            "2023-05-07 13:34:58,589 - INFO - joeynmt.training - EPOCH 68\n",
            "2023-05-07 13:34:59,565 - INFO - joeynmt.training - Epoch  68, Step:      350, Batch Loss:     4.402589, Tokens per Sec:     3516, Lr: 0.000300\n",
            "2023-05-07 13:34:59,734 - INFO - joeynmt.training - Epoch  68: total training loss 21.61\n",
            "2023-05-07 13:34:59,734 - INFO - joeynmt.training - EPOCH 69\n",
            "2023-05-07 13:35:00,911 - INFO - joeynmt.training - Epoch  69: total training loss 21.42\n",
            "2023-05-07 13:35:00,911 - INFO - joeynmt.training - EPOCH 70\n",
            "2023-05-07 13:35:02,041 - INFO - joeynmt.training - Epoch  70: total training loss 21.32\n",
            "2023-05-07 13:35:02,041 - INFO - joeynmt.training - EPOCH 71\n",
            "2023-05-07 13:35:03,139 - INFO - joeynmt.training - Epoch  71: total training loss 21.07\n",
            "2023-05-07 13:35:03,139 - INFO - joeynmt.training - EPOCH 72\n",
            "2023-05-07 13:35:04,268 - INFO - joeynmt.training - Epoch  72: total training loss 21.05\n",
            "2023-05-07 13:35:04,268 - INFO - joeynmt.training - EPOCH 73\n",
            "2023-05-07 13:35:05,316 - INFO - joeynmt.training - Epoch  73: total training loss 20.83\n",
            "2023-05-07 13:35:05,317 - INFO - joeynmt.training - EPOCH 74\n",
            "2023-05-07 13:35:06,428 - INFO - joeynmt.training - Epoch  74: total training loss 24.88\n",
            "2023-05-07 13:35:06,428 - INFO - joeynmt.training - EPOCH 75\n",
            "2023-05-07 13:35:07,529 - INFO - joeynmt.training - Epoch  75: total training loss 20.56\n",
            "2023-05-07 13:35:07,530 - INFO - joeynmt.training - EPOCH 76\n",
            "2023-05-07 13:35:08,688 - INFO - joeynmt.training - Epoch  76: total training loss 24.32\n",
            "2023-05-07 13:35:08,688 - INFO - joeynmt.training - EPOCH 77\n",
            "2023-05-07 13:35:09,746 - INFO - joeynmt.training - Epoch  77: total training loss 20.49\n",
            "2023-05-07 13:35:09,747 - INFO - joeynmt.training - EPOCH 78\n",
            "2023-05-07 13:35:10,221 - INFO - joeynmt.training - Epoch  78, Step:      400, Batch Loss:     4.087845, Tokens per Sec:     3709, Lr: 0.000300\n",
            "2023-05-07 13:35:40,086 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tHypothesis: - - no.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tHypothesis: no el va va teva teva teva teva fer això.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tHypothesis: - - - ho bé.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:35:40,087 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:35:40,088 - INFO - joeynmt.training - \tHypothesis: sí.\n",
            "2023-05-07 13:35:40,088 - INFO - joeynmt.training - Validation result (greedy) at epoch  78, step      400: bleu:   0.03, loss: 29929.5078, ppl: 1020.5804, duration: 29.8667s\n",
            "2023-05-07 13:35:40,723 - INFO - joeynmt.training - Epoch  78: total training loss 20.37\n",
            "2023-05-07 13:35:40,723 - INFO - joeynmt.training - EPOCH 79\n",
            "2023-05-07 13:35:41,857 - INFO - joeynmt.training - Epoch  79: total training loss 20.15\n",
            "2023-05-07 13:35:41,857 - INFO - joeynmt.training - EPOCH 80\n",
            "2023-05-07 13:35:42,924 - INFO - joeynmt.training - Epoch  80: total training loss 20.01\n",
            "2023-05-07 13:35:42,924 - INFO - joeynmt.training - EPOCH 81\n",
            "2023-05-07 13:35:44,081 - INFO - joeynmt.training - Epoch  81: total training loss 20.22\n",
            "2023-05-07 13:35:44,082 - INFO - joeynmt.training - EPOCH 82\n",
            "2023-05-07 13:35:45,290 - INFO - joeynmt.training - Epoch  82: total training loss 19.99\n",
            "2023-05-07 13:35:45,291 - INFO - joeynmt.training - EPOCH 83\n",
            "2023-05-07 13:35:46,317 - INFO - joeynmt.training - Epoch  83: total training loss 19.84\n",
            "2023-05-07 13:35:46,318 - INFO - joeynmt.training - EPOCH 84\n",
            "2023-05-07 13:35:47,419 - INFO - joeynmt.training - Epoch  84: total training loss 19.60\n",
            "2023-05-07 13:35:47,419 - INFO - joeynmt.training - EPOCH 85\n",
            "2023-05-07 13:35:48,649 - INFO - joeynmt.training - Epoch  85: total training loss 19.39\n",
            "2023-05-07 13:35:48,650 - INFO - joeynmt.training - EPOCH 86\n",
            "2023-05-07 13:35:49,758 - INFO - joeynmt.training - Epoch  86: total training loss 19.27\n",
            "2023-05-07 13:35:49,758 - INFO - joeynmt.training - EPOCH 87\n",
            "2023-05-07 13:35:50,890 - INFO - joeynmt.training - Epoch  87: total training loss 19.16\n",
            "2023-05-07 13:35:50,890 - INFO - joeynmt.training - EPOCH 88\n",
            "2023-05-07 13:35:51,388 - INFO - joeynmt.training - Epoch  88, Step:      450, Batch Loss:     3.785204, Tokens per Sec:     3233, Lr: 0.000300\n",
            "2023-05-07 13:35:52,026 - INFO - joeynmt.training - Epoch  88: total training loss 18.94\n",
            "2023-05-07 13:35:52,026 - INFO - joeynmt.training - EPOCH 89\n",
            "2023-05-07 13:35:53,242 - INFO - joeynmt.training - Epoch  89: total training loss 18.84\n",
            "2023-05-07 13:35:53,242 - INFO - joeynmt.training - EPOCH 90\n",
            "2023-05-07 13:35:54,394 - INFO - joeynmt.training - Epoch  90: total training loss 18.74\n",
            "2023-05-07 13:35:54,394 - INFO - joeynmt.training - EPOCH 91\n",
            "2023-05-07 13:35:55,478 - INFO - joeynmt.training - Epoch  91: total training loss 18.50\n",
            "2023-05-07 13:35:55,478 - INFO - joeynmt.training - EPOCH 92\n",
            "2023-05-07 13:35:56,564 - INFO - joeynmt.training - Epoch  92: total training loss 18.33\n",
            "2023-05-07 13:35:56,564 - INFO - joeynmt.training - EPOCH 93\n",
            "2023-05-07 13:35:57,760 - INFO - joeynmt.training - Epoch  93: total training loss 18.35\n",
            "2023-05-07 13:35:57,760 - INFO - joeynmt.training - EPOCH 94\n",
            "2023-05-07 13:35:58,852 - INFO - joeynmt.training - Epoch  94: total training loss 18.25\n",
            "2023-05-07 13:35:58,852 - INFO - joeynmt.training - EPOCH 95\n",
            "2023-05-07 13:35:59,963 - INFO - joeynmt.training - Epoch  95: total training loss 18.08\n",
            "2023-05-07 13:35:59,963 - INFO - joeynmt.training - EPOCH 96\n",
            "2023-05-07 13:36:00,946 - INFO - joeynmt.training - Epoch  96: total training loss 17.95\n",
            "2023-05-07 13:36:00,946 - INFO - joeynmt.training - EPOCH 97\n",
            "2023-05-07 13:36:02,035 - INFO - joeynmt.training - Epoch  97: total training loss 17.82\n",
            "2023-05-07 13:36:02,036 - INFO - joeynmt.training - EPOCH 98\n",
            "2023-05-07 13:36:02,533 - INFO - joeynmt.training - Epoch  98, Step:      500, Batch Loss:     3.556955, Tokens per Sec:     3283, Lr: 0.000300\n",
            "2023-05-07 13:36:33,147 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:36:33,147 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:36:33,147 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:36:33,147 - INFO - joeynmt.training - \tHypothesis: què sé.\n",
            "2023-05-07 13:36:33,147 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tHypothesis: no va pensque la teva teva això.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tHypothesis: - - ho sé.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - \tHypothesis: sí.\n",
            "2023-05-07 13:36:33,148 - INFO - joeynmt.training - Validation result (greedy) at epoch  98, step      500: bleu:   0.08, loss: 30865.2246, ppl: 1267.4062, duration: 30.6148s\n",
            "2023-05-07 13:36:33,847 - INFO - joeynmt.training - Epoch  98: total training loss 17.66\n",
            "2023-05-07 13:36:33,847 - INFO - joeynmt.training - EPOCH 99\n",
            "2023-05-07 13:36:35,000 - INFO - joeynmt.training - Epoch  99: total training loss 17.53\n",
            "2023-05-07 13:36:35,000 - INFO - joeynmt.training - EPOCH 100\n",
            "2023-05-07 13:36:36,183 - INFO - joeynmt.training - Epoch 100: total training loss 17.42\n",
            "2023-05-07 13:36:36,183 - INFO - joeynmt.training - EPOCH 101\n",
            "2023-05-07 13:36:37,487 - INFO - joeynmt.training - Epoch 101: total training loss 20.94\n",
            "2023-05-07 13:36:37,487 - INFO - joeynmt.training - EPOCH 102\n",
            "2023-05-07 13:36:38,621 - INFO - joeynmt.training - Epoch 102: total training loss 17.18\n",
            "2023-05-07 13:36:38,622 - INFO - joeynmt.training - EPOCH 103\n",
            "2023-05-07 13:36:39,856 - INFO - joeynmt.training - Epoch 103: total training loss 17.04\n",
            "2023-05-07 13:36:39,856 - INFO - joeynmt.training - EPOCH 104\n",
            "2023-05-07 13:36:40,978 - INFO - joeynmt.training - Epoch 104: total training loss 16.88\n",
            "2023-05-07 13:36:40,979 - INFO - joeynmt.training - EPOCH 105\n",
            "2023-05-07 13:36:42,195 - INFO - joeynmt.training - Epoch 105: total training loss 16.74\n",
            "2023-05-07 13:36:42,195 - INFO - joeynmt.training - EPOCH 106\n",
            "2023-05-07 13:36:43,193 - INFO - joeynmt.training - Epoch 106: total training loss 16.62\n",
            "2023-05-07 13:36:43,193 - INFO - joeynmt.training - EPOCH 107\n",
            "2023-05-07 13:36:44,394 - INFO - joeynmt.training - Epoch 107, Step:      550, Batch Loss:     3.023309, Tokens per Sec:     3530, Lr: 0.000300\n",
            "2023-05-07 13:36:44,395 - INFO - joeynmt.training - Epoch 107: total training loss 19.48\n",
            "2023-05-07 13:36:44,395 - INFO - joeynmt.training - EPOCH 108\n",
            "2023-05-07 13:36:45,521 - INFO - joeynmt.training - Epoch 108: total training loss 16.71\n",
            "2023-05-07 13:36:45,522 - INFO - joeynmt.training - EPOCH 109\n",
            "2023-05-07 13:36:46,676 - INFO - joeynmt.training - Epoch 109: total training loss 16.43\n",
            "2023-05-07 13:36:46,676 - INFO - joeynmt.training - EPOCH 110\n",
            "2023-05-07 13:36:47,671 - INFO - joeynmt.training - Epoch 110: total training loss 16.39\n",
            "2023-05-07 13:36:47,672 - INFO - joeynmt.training - EPOCH 111\n",
            "2023-05-07 13:36:48,885 - INFO - joeynmt.training - Epoch 111: total training loss 16.06\n",
            "2023-05-07 13:36:48,885 - INFO - joeynmt.training - EPOCH 112\n",
            "2023-05-07 13:36:50,156 - INFO - joeynmt.training - Epoch 112: total training loss 19.09\n",
            "2023-05-07 13:36:50,156 - INFO - joeynmt.training - EPOCH 113\n",
            "2023-05-07 13:36:51,388 - INFO - joeynmt.training - Epoch 113: total training loss 18.87\n",
            "2023-05-07 13:36:51,388 - INFO - joeynmt.training - EPOCH 114\n",
            "2023-05-07 13:36:52,469 - INFO - joeynmt.training - Epoch 114: total training loss 15.66\n",
            "2023-05-07 13:36:52,470 - INFO - joeynmt.training - EPOCH 115\n",
            "2023-05-07 13:36:53,646 - INFO - joeynmt.training - Epoch 115: total training loss 15.51\n",
            "2023-05-07 13:36:53,647 - INFO - joeynmt.training - EPOCH 116\n",
            "2023-05-07 13:36:54,792 - INFO - joeynmt.training - Epoch 116: total training loss 15.36\n",
            "2023-05-07 13:36:54,792 - INFO - joeynmt.training - EPOCH 117\n",
            "2023-05-07 13:36:55,504 - INFO - joeynmt.training - Epoch 117, Step:      600, Batch Loss:     3.088326, Tokens per Sec:     4172, Lr: 0.000300\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tHypothesis: què fas?\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - \tHypothesis: no pots que les teva bons.\n",
            "2023-05-07 13:37:24,233 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tHypothesis: - què fas?\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - \tHypothesis: què fas?\n",
            "2023-05-07 13:37:24,234 - INFO - joeynmt.training - Validation result (greedy) at epoch 117, step      600: bleu:   0.12, loss: 31309.9121, ppl: 1404.8207, duration: 28.7301s\n",
            "2023-05-07 13:37:24,632 - INFO - joeynmt.training - Epoch 117: total training loss 15.21\n",
            "2023-05-07 13:37:24,633 - INFO - joeynmt.training - EPOCH 118\n",
            "2023-05-07 13:37:25,825 - INFO - joeynmt.training - Epoch 118: total training loss 15.05\n",
            "2023-05-07 13:37:25,825 - INFO - joeynmt.training - EPOCH 119\n",
            "2023-05-07 13:37:27,118 - INFO - joeynmt.training - Epoch 119: total training loss 18.03\n",
            "2023-05-07 13:37:27,118 - INFO - joeynmt.training - EPOCH 120\n",
            "2023-05-07 13:37:28,231 - INFO - joeynmt.training - Epoch 120: total training loss 15.00\n",
            "2023-05-07 13:37:28,231 - INFO - joeynmt.training - EPOCH 121\n",
            "2023-05-07 13:37:29,408 - INFO - joeynmt.training - Epoch 121: total training loss 17.83\n",
            "2023-05-07 13:37:29,408 - INFO - joeynmt.training - EPOCH 122\n",
            "2023-05-07 13:37:30,583 - INFO - joeynmt.training - Epoch 122: total training loss 14.74\n",
            "2023-05-07 13:37:30,583 - INFO - joeynmt.training - EPOCH 123\n",
            "2023-05-07 13:37:31,771 - INFO - joeynmt.training - Epoch 123: total training loss 17.43\n",
            "2023-05-07 13:37:31,771 - INFO - joeynmt.training - EPOCH 124\n",
            "2023-05-07 13:37:32,960 - INFO - joeynmt.training - Epoch 124: total training loss 16.59\n",
            "2023-05-07 13:37:32,960 - INFO - joeynmt.training - EPOCH 125\n",
            "2023-05-07 13:37:34,146 - INFO - joeynmt.training - Epoch 125: total training loss 14.48\n",
            "2023-05-07 13:37:34,146 - INFO - joeynmt.training - EPOCH 126\n",
            "2023-05-07 13:37:35,167 - INFO - joeynmt.training - Epoch 126, Step:      650, Batch Loss:     2.890115, Tokens per Sec:     4009, Lr: 0.000300\n",
            "2023-05-07 13:37:35,237 - INFO - joeynmt.training - Epoch 126: total training loss 14.37\n",
            "2023-05-07 13:37:35,237 - INFO - joeynmt.training - EPOCH 127\n",
            "2023-05-07 13:37:36,493 - INFO - joeynmt.training - Epoch 127: total training loss 17.00\n",
            "2023-05-07 13:37:36,493 - INFO - joeynmt.training - EPOCH 128\n",
            "2023-05-07 13:37:37,723 - INFO - joeynmt.training - Epoch 128: total training loss 14.12\n",
            "2023-05-07 13:37:37,723 - INFO - joeynmt.training - EPOCH 129\n",
            "2023-05-07 13:37:38,846 - INFO - joeynmt.training - Epoch 129: total training loss 14.03\n",
            "2023-05-07 13:37:38,846 - INFO - joeynmt.training - EPOCH 130\n",
            "2023-05-07 13:37:39,998 - INFO - joeynmt.training - Epoch 130: total training loss 13.73\n",
            "2023-05-07 13:37:39,999 - INFO - joeynmt.training - EPOCH 131\n",
            "2023-05-07 13:37:41,145 - INFO - joeynmt.training - Epoch 131: total training loss 13.61\n",
            "2023-05-07 13:37:41,146 - INFO - joeynmt.training - EPOCH 132\n",
            "2023-05-07 13:37:42,362 - INFO - joeynmt.training - Epoch 132: total training loss 13.38\n",
            "2023-05-07 13:37:42,363 - INFO - joeynmt.training - EPOCH 133\n",
            "2023-05-07 13:37:43,424 - INFO - joeynmt.training - Epoch 133: total training loss 13.25\n",
            "2023-05-07 13:37:43,424 - INFO - joeynmt.training - EPOCH 134\n",
            "2023-05-07 13:37:44,568 - INFO - joeynmt.training - Epoch 134: total training loss 15.77\n",
            "2023-05-07 13:37:44,569 - INFO - joeynmt.training - EPOCH 135\n",
            "2023-05-07 13:37:45,623 - INFO - joeynmt.training - Epoch 135: total training loss 12.92\n",
            "2023-05-07 13:37:45,623 - INFO - joeynmt.training - EPOCH 136\n",
            "2023-05-07 13:37:46,099 - INFO - joeynmt.training - Epoch 136, Step:      700, Batch Loss:     2.592119, Tokens per Sec:     3221, Lr: 0.000300\n",
            "2023-05-07 13:38:14,769 - INFO - joeynmt.training - Example #0\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tSource:     well. what happened to you?\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tReference:  què t'ha passat?\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tHypothesis: què fas?\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - Example #1\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tSource:     that's even more fun than gee... ode.\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tReference:  és encara més divertit que ge... oda.\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tHypothesis: vull ha que fer una fer això.\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - Example #2\n",
            "2023-05-07 13:38:14,770 - INFO - joeynmt.training - \tSource:     - but andrea...\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - \tReference:  - però n'andrea...\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - \tHypothesis: - un problema.\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - Example #3\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - \tSource:     we've moved.\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - \tReference:  ens hem mogut.\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - \tHypothesis: tens es?\n",
            "2023-05-07 13:38:14,771 - INFO - joeynmt.training - Validation result (greedy) at epoch 136, step      700: bleu:   0.17, loss: 32336.9434, ppl: 1781.8431, duration: 28.6717s\n",
            "2023-05-07 13:38:15,585 - INFO - joeynmt.training - Epoch 136: total training loss 15.65\n",
            "2023-05-07 13:38:15,585 - INFO - joeynmt.training - EPOCH 137\n",
            "2023-05-07 13:38:16,786 - INFO - joeynmt.training - Epoch 137: total training loss 12.87\n",
            "2023-05-07 13:38:16,786 - INFO - joeynmt.training - EPOCH 138\n",
            "2023-05-07 13:38:17,836 - INFO - joeynmt.training - Epoch 138: total training loss 12.65\n",
            "2023-05-07 13:38:17,837 - INFO - joeynmt.training - EPOCH 139\n",
            "2023-05-07 13:38:18,988 - INFO - joeynmt.training - Epoch 139: total training loss 12.64\n",
            "2023-05-07 13:38:18,989 - INFO - joeynmt.training - EPOCH 140\n",
            "2023-05-07 13:38:20,159 - INFO - joeynmt.training - Epoch 140: total training loss 12.48\n",
            "2023-05-07 13:38:20,159 - INFO - joeynmt.training - EPOCH 141\n",
            "2023-05-07 13:38:21,331 - INFO - joeynmt.training - Epoch 141: total training loss 14.95\n",
            "2023-05-07 13:38:21,332 - INFO - joeynmt.training - EPOCH 142\n",
            "2023-05-07 13:38:22,620 - INFO - joeynmt.training - Epoch 142: total training loss 14.82\n",
            "2023-05-07 13:38:22,620 - INFO - joeynmt.training - EPOCH 143\n",
            "2023-05-07 13:38:23,700 - INFO - joeynmt.training - Epoch 143: total training loss 12.08\n",
            "2023-05-07 13:38:23,700 - INFO - joeynmt.training - EPOCH 144\n",
            "2023-05-07 13:38:24,765 - INFO - joeynmt.training - Epoch 144: total training loss 11.93\n",
            "2023-05-07 13:38:24,765 - INFO - joeynmt.training - EPOCH 145\n",
            "2023-05-07 13:38:25,719 - INFO - joeynmt.training - Epoch 145, Step:      750, Batch Loss:     2.308299, Tokens per Sec:     3757, Lr: 0.000300\n",
            "2023-05-07 13:38:25,904 - INFO - joeynmt.training - Epoch 145: total training loss 11.86\n",
            "2023-05-07 13:38:25,905 - INFO - joeynmt.training - EPOCH 146\n",
            "2023-05-07 13:38:26,859 - INFO - joeynmt.training - Epoch 146: total training loss 9.38\n",
            "2023-05-07 13:38:26,860 - INFO - joeynmt.training - EPOCH 147\n",
            "2023-05-07 13:38:27,954 - INFO - joeynmt.training - Epoch 147: total training loss 11.56\n",
            "2023-05-07 13:38:27,954 - INFO - joeynmt.training - EPOCH 148\n",
            "2023-05-07 13:38:29,187 - INFO - joeynmt.training - Epoch 148: total training loss 13.86\n",
            "2023-05-07 13:38:29,187 - INFO - joeynmt.training - EPOCH 149\n",
            "2023-05-07 13:38:30,258 - INFO - joeynmt.training - Epoch 149: total training loss 11.45\n",
            "2023-05-07 13:38:30,258 - INFO - joeynmt.training - EPOCH 150\n",
            "2023-05-07 13:38:31,367 - INFO - joeynmt.training - Epoch 150: total training loss 11.27\n",
            "2023-05-07 13:38:31,368 - INFO - joeynmt.training - EPOCH 151\n",
            "2023-05-07 13:38:32,537 - INFO - joeynmt.training - Epoch 151: total training loss 11.11\n",
            "2023-05-07 13:38:32,537 - INFO - joeynmt.training - EPOCH 152\n",
            "2023-05-07 13:38:33,664 - INFO - joeynmt.training - Epoch 152: total training loss 10.99\n",
            "2023-05-07 13:38:33,665 - INFO - joeynmt.training - EPOCH 153\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBoDS09JM807"
      },
      "outputs": [],
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistent storage \n",
        "! mkdir -p \"$data_path/models/${src}${tgt}_transformer/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$data_path/models/${src}${tgt}_transformer/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9568d2dd-9afd-4dbd-8f1e-08780add582e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 100\tLoss: 13694.73730\tPPL: 135.43716\tbleu: 0.05823\tLR: 0.00030000\t*\n",
            "Steps: 200\tLoss: 12679.11426\tPPL: 94.11195\tbleu: 0.01495\tLR: 0.00030000\t*\n",
            "Steps: 300\tLoss: 12329.75586\tPPL: 83.03542\tbleu: 0.09308\tLR: 0.00030000\t*\n",
            "Steps: 400\tLoss: 12154.72070\tPPL: 77.98611\tbleu: 0.45976\tLR: 0.00030000\t*\n",
            "Steps: 500\tLoss: 12022.83496\tPPL: 74.38541\tbleu: 0.80195\tLR: 0.00030000\t*\n",
            "Steps: 600\tLoss: 11848.84570\tPPL: 69.88830\tbleu: 1.19764\tLR: 0.00030000\t*\n",
            "Steps: 700\tLoss: 11860.50391\tPPL: 70.18093\tbleu: 1.24440\tLR: 0.00030000\t\n",
            "Steps: 800\tLoss: 11740.67090\tPPL: 67.23040\tbleu: 1.25418\tLR: 0.00030000\t*\n",
            "Steps: 900\tLoss: 11781.74023\tPPL: 68.22739\tbleu: 1.66570\tLR: 0.00030000\t\n",
            "Steps: 1000\tLoss: 11865.29102\tPPL: 70.30145\tbleu: 1.66643\tLR: 0.00030000\t\n",
            "Steps: 1100\tLoss: 12116.89551\tPPL: 76.93595\tbleu: 1.55163\tLR: 0.00030000\t\n",
            "Steps: 1200\tLoss: 12060.12793\tPPL: 75.38638\tbleu: 1.57219\tLR: 0.00030000\t\n",
            "Steps: 1300\tLoss: 12393.08203\tPPL: 84.94166\tbleu: 1.40682\tLR: 0.00030000\t\n",
            "Steps: 1400\tLoss: 12249.62988\tPPL: 80.68464\tbleu: 1.99889\tLR: 0.00021000\t\n",
            "Steps: 1500\tLoss: 12466.73242\tPPL: 87.21382\tbleu: 1.67914\tLR: 0.00021000\t\n",
            "Steps: 1600\tLoss: 12534.74023\tPPL: 89.36581\tbleu: 1.82400\tLR: 0.00021000\t\n",
            "Steps: 1700\tLoss: 12717.83398\tPPL: 95.42718\tbleu: 1.77747\tLR: 0.00021000\t\n",
            "Steps: 1800\tLoss: 12743.17383\tPPL: 96.29783\tbleu: 2.07937\tLR: 0.00021000\t\n",
            "Steps: 1900\tLoss: 12974.76074\tPPL: 104.63223\tbleu: 2.48006\tLR: 0.00021000\t\n",
            "Steps: 2000\tLoss: 13008.83008\tPPL: 105.91776\tbleu: 2.08245\tLR: 0.00014700\t\n",
            "Steps: 2100\tLoss: 13061.74316\tPPL: 107.94569\tbleu: 2.22271\tLR: 0.00014700\t\n",
            "Steps: 2200\tLoss: 13242.54492\tPPL: 115.17259\tbleu: 2.45243\tLR: 0.00014700\t\n",
            "Steps: 2300\tLoss: 13366.31641\tPPL: 120.39697\tbleu: 2.39776\tLR: 0.00014700\t\n",
            "Steps: 2400\tLoss: 13374.03711\tPPL: 120.73058\tbleu: 2.38867\tLR: 0.00014700\t\n",
            "Steps: 2500\tLoss: 13519.73340\tPPL: 127.20274\tbleu: 2.25657\tLR: 0.00014700\t\n",
            "Steps: 2600\tLoss: 13514.95020\tPPL: 126.98488\tbleu: 2.60089\tLR: 0.00010290\t\n",
            "Steps: 2700\tLoss: 13650.81543\tPPL: 133.32170\tbleu: 2.71637\tLR: 0.00010290\t\n",
            "Steps: 2800\tLoss: 13681.58398\tPPL: 134.80016\tbleu: 2.87417\tLR: 0.00010290\t\n",
            "Steps: 2900\tLoss: 13897.38086\tPPL: 145.64034\tbleu: 2.55185\tLR: 0.00010290\t\n",
            "Steps: 3000\tLoss: 13919.22852\tPPL: 146.78526\tbleu: 2.48261\tLR: 0.00010290\t\n",
            "Steps: 3100\tLoss: 13922.39648\tPPL: 146.95201\tbleu: 2.78884\tLR: 0.00010290\t\n",
            "Steps: 3200\tLoss: 14064.68164\tPPL: 154.64069\tbleu: 2.52079\tLR: 0.00007203\t\n",
            "Steps: 3300\tLoss: 14083.10059\tPPL: 155.66498\tbleu: 2.64790\tLR: 0.00007203\t\n",
            "Steps: 3400\tLoss: 14146.83789\tPPL: 159.26204\tbleu: 2.55328\tLR: 0.00007203\t\n",
            "Steps: 3500\tLoss: 14221.12012\tPPL: 163.55930\tbleu: 2.59238\tLR: 0.00007203\t\n",
            "Steps: 3600\tLoss: 14289.05469\tPPL: 167.59071\tbleu: 2.49434\tLR: 0.00007203\t\n",
            "Steps: 3700\tLoss: 14257.03125\tPPL: 165.67813\tbleu: 2.51564\tLR: 0.00007203\t\n",
            "Steps: 3800\tLoss: 14348.21875\tPPL: 171.18257\tbleu: 2.48025\tLR: 0.00005042\t\n",
            "Steps: 3900\tLoss: 14407.99805\tPPL: 174.88995\tbleu: 2.59335\tLR: 0.00005042\t\n",
            "Steps: 4000\tLoss: 14382.33887\tPPL: 173.28891\tbleu: 2.60481\tLR: 0.00005042\t\n",
            "Steps: 4100\tLoss: 14464.56543\tPPL: 178.47205\tbleu: 2.74171\tLR: 0.00005042\t\n",
            "Steps: 4200\tLoss: 14468.81445\tPPL: 178.74406\tbleu: 3.16740\tLR: 0.00005042\t\n",
            "Steps: 4300\tLoss: 14552.02344\tPPL: 184.15520\tbleu: 2.78056\tLR: 0.00005042\t\n",
            "Steps: 4400\tLoss: 14539.53711\tPPL: 183.33282\tbleu: 2.64308\tLR: 0.00003529\t\n",
            "Steps: 4500\tLoss: 14614.85059\tPPL: 188.34917\tbleu: 2.65470\tLR: 0.00003529\t\n",
            "Steps: 4600\tLoss: 14643.62793\tPPL: 190.30193\tbleu: 2.81514\tLR: 0.00003529\t\n",
            "Steps: 4700\tLoss: 14622.75000\tPPL: 188.88322\tbleu: 2.78711\tLR: 0.00003529\t\n",
            "Steps: 4800\tLoss: 14667.30078\tPPL: 191.92348\tbleu: 2.67023\tLR: 0.00003529\t\n",
            "Steps: 4900\tLoss: 14676.44434\tPPL: 192.55350\tbleu: 2.74311\tLR: 0.00003529\t\n",
            "Steps: 5000\tLoss: 14699.17578\tPPL: 194.12878\tbleu: 3.01704\tLR: 0.00002471\t\n",
            "Steps: 5100\tLoss: 14752.95508\tPPL: 197.90704\tbleu: 3.10422\tLR: 0.00002471\t\n",
            "Steps: 5200\tLoss: 14749.58203\tPPL: 197.66786\tbleu: 2.74776\tLR: 0.00002471\t\n",
            "Steps: 5300\tLoss: 14803.64062\tPPL: 201.53519\tbleu: 2.58333\tLR: 0.00002471\t\n",
            "Steps: 5400\tLoss: 14832.26172\tPPL: 203.61331\tbleu: 2.95410\tLR: 0.00002471\t\n",
            "Steps: 5500\tLoss: 14807.17383\tPPL: 201.79059\tbleu: 2.93037\tLR: 0.00002471\t\n",
            "Steps: 5600\tLoss: 14809.99121\tPPL: 201.99448\tbleu: 2.62011\tLR: 0.00001729\t\n",
            "Steps: 5700\tLoss: 14826.80859\tPPL: 203.21573\tbleu: 2.92478\tLR: 0.00001729\t\n",
            "Steps: 5800\tLoss: 14815.81250\tPPL: 202.41641\tbleu: 2.90560\tLR: 0.00001729\t\n",
            "Steps: 5900\tLoss: 14822.36816\tPPL: 202.89253\tbleu: 2.47944\tLR: 0.00001729\t\n",
            "Steps: 6000\tLoss: 14848.00586\tPPL: 204.76553\tbleu: 2.65660\tLR: 0.00001729\t\n",
            "Steps: 6100\tLoss: 14885.82812\tPPL: 207.56033\tbleu: 2.99469\tLR: 0.00001729\t\n",
            "Steps: 6200\tLoss: 14876.01953\tPPL: 206.83188\tbleu: 2.68096\tLR: 0.00001211\t\n",
            "Steps: 6300\tLoss: 14905.07812\tPPL: 208.99741\tbleu: 2.75716\tLR: 0.00001211\t\n",
            "Steps: 6400\tLoss: 14899.69336\tPPL: 208.59438\tbleu: 2.67755\tLR: 0.00001211\t\n",
            "Steps: 6500\tLoss: 14885.01172\tPPL: 207.49957\tbleu: 2.77681\tLR: 0.00001211\t\n",
            "Steps: 6600\tLoss: 14913.99609\tPPL: 209.66649\tbleu: 2.73355\tLR: 0.00001211\t\n",
            "Steps: 6700\tLoss: 14933.18945\tPPL: 211.11383\tbleu: 2.70943\tLR: 0.00001211\t\n",
            "Steps: 6800\tLoss: 14924.61719\tPPL: 210.46623\tbleu: 2.67218\tLR: 0.00000847\t\n",
            "Steps: 6900\tLoss: 14958.49609\tPPL: 213.03743\tbleu: 2.64454\tLR: 0.00000847\t\n",
            "Steps: 7000\tLoss: 14954.41309\tPPL: 212.72589\tbleu: 2.69487\tLR: 0.00000847\t\n",
            "Steps: 7100\tLoss: 14952.91992\tPPL: 212.61211\tbleu: 2.71378\tLR: 0.00000847\t\n",
            "Steps: 7200\tLoss: 14944.49707\tPPL: 211.97122\tbleu: 2.76175\tLR: 0.00000847\t\n",
            "Steps: 7300\tLoss: 14957.83008\tPPL: 212.98663\tbleu: 2.67828\tLR: 0.00000847\t\n",
            "Steps: 7400\tLoss: 14976.62305\tPPL: 214.42610\tbleu: 2.64740\tLR: 0.00000593\t\n",
            "Steps: 7500\tLoss: 14985.48633\tPPL: 215.10835\tbleu: 2.78814\tLR: 0.00000593\t\n",
            "Steps: 7600\tLoss: 14999.98242\tPPL: 216.22890\tbleu: 2.67498\tLR: 0.00000593\t\n",
            "Steps: 7700\tLoss: 14992.48633\tPPL: 215.64877\tbleu: 2.72178\tLR: 0.00000593\t\n",
            "Steps: 7800\tLoss: 14994.80273\tPPL: 215.82787\tbleu: 2.70406\tLR: 0.00000593\t\n",
            "Steps: 7900\tLoss: 14986.30762\tPPL: 215.17166\tbleu: 2.70642\tLR: 0.00000593\t\n",
            "Steps: 8000\tLoss: 15001.01465\tPPL: 216.30891\tbleu: 2.70903\tLR: 0.00000415\t\n",
            "Steps: 8100\tLoss: 14994.89844\tPPL: 215.83527\tbleu: 2.73336\tLR: 0.00000415\t\n",
            "Steps: 8200\tLoss: 15006.90039\tPPL: 216.76572\tbleu: 2.80362\tLR: 0.00000415\t\n",
            "Steps: 8300\tLoss: 15019.24414\tPPL: 217.72693\tbleu: 2.86878\tLR: 0.00000415\t\n",
            "Steps: 8400\tLoss: 15011.67188\tPPL: 217.13678\tbleu: 2.68043\tLR: 0.00000415\t\n",
            "Steps: 8500\tLoss: 15017.43164\tPPL: 217.58546\tbleu: 2.79155\tLR: 0.00000415\t\n",
            "Steps: 8600\tLoss: 15023.86523\tPPL: 218.08780\tbleu: 2.77694\tLR: 0.00000291\t\n",
            "Steps: 8700\tLoss: 15014.34570\tPPL: 217.34500\tbleu: 2.79720\tLR: 0.00000291\t\n",
            "Steps: 8800\tLoss: 15024.46875\tPPL: 218.13501\tbleu: 2.75097\tLR: 0.00000291\t\n",
            "Steps: 8900\tLoss: 15019.00879\tPPL: 217.70854\tbleu: 3.03852\tLR: 0.00000291\t\n",
            "Steps: 9000\tLoss: 15018.36523\tPPL: 217.65831\tbleu: 3.18700\tLR: 0.00000291\t\n",
            "Steps: 9100\tLoss: 15025.91797\tPPL: 218.24831\tbleu: 3.10294\tLR: 0.00000291\t\n",
            "Steps: 9200\tLoss: 15023.44531\tPPL: 218.05493\tbleu: 3.13450\tLR: 0.00000203\t\n",
            "Steps: 9300\tLoss: 15029.92578\tPPL: 218.56209\tbleu: 3.09978\tLR: 0.00000203\t\n",
            "Steps: 9400\tLoss: 15031.08594\tPPL: 218.65298\tbleu: 3.05096\tLR: 0.00000203\t\n",
            "Steps: 9500\tLoss: 15035.01562\tPPL: 218.96109\tbleu: 3.11741\tLR: 0.00000203\t\n",
            "Steps: 9600\tLoss: 15032.15234\tPPL: 218.73653\tbleu: 3.06219\tLR: 0.00000203\t\n",
            "Steps: 9700\tLoss: 15039.59180\tPPL: 219.32056\tbleu: 2.68462\tLR: 0.00000203\t\n",
            "Steps: 9800\tLoss: 15035.08984\tPPL: 218.96695\tbleu: 3.05009\tLR: 0.00000142\t\n",
            "Steps: 9900\tLoss: 15047.41602\tPPL: 219.93646\tbleu: 2.79557\tLR: 0.00000142\t\n",
            "Steps: 10000\tLoss: 15040.97949\tPPL: 219.42966\tbleu: 3.09070\tLR: 0.00000142\t\n",
            "Steps: 10100\tLoss: 15042.20508\tPPL: 219.52617\tbleu: 3.09492\tLR: 0.00000142\t\n",
            "Steps: 10200\tLoss: 15036.15820\tPPL: 219.05081\tbleu: 3.06496\tLR: 0.00000142\t\n",
            "Steps: 10300\tLoss: 15037.71484\tPPL: 219.17303\tbleu: 3.06023\tLR: 0.00000142\t\n",
            "Steps: 10400\tLoss: 15038.97363\tPPL: 219.27203\tbleu: 3.23769\tLR: 0.00000100\t\n",
            "Steps: 10500\tLoss: 15042.97656\tPPL: 219.58678\tbleu: 3.10225\tLR: 0.00000100\t\n",
            "Steps: 10600\tLoss: 15051.21875\tPPL: 220.23650\tbleu: 2.81423\tLR: 0.00000100\t\n",
            "Steps: 10700\tLoss: 15050.05469\tPPL: 220.14462\tbleu: 2.72506\tLR: 0.00000100\t\n",
            "Steps: 10800\tLoss: 15049.42578\tPPL: 220.09499\tbleu: 3.08176\tLR: 0.00000100\t\n",
            "Steps: 10900\tLoss: 15055.08984\tPPL: 220.54231\tbleu: 2.80671\tLR: 0.00000100\t\n",
            "Steps: 11000\tLoss: 15052.91992\tPPL: 220.37085\tbleu: 3.06993\tLR: 0.00000070\t\n",
            "Steps: 11100\tLoss: 15048.08105\tPPL: 219.98889\tbleu: 3.13712\tLR: 0.00000070\t\n",
            "Steps: 11200\tLoss: 15045.16699\tPPL: 219.75929\tbleu: 2.81467\tLR: 0.00000070\t\n",
            "Steps: 11300\tLoss: 15048.69043\tPPL: 220.03694\tbleu: 2.79531\tLR: 0.00000070\t\n",
            "Steps: 11400\tLoss: 15043.30664\tPPL: 219.61285\tbleu: 2.80103\tLR: 0.00000070\t\n",
            "Steps: 11500\tLoss: 15046.45703\tPPL: 219.86096\tbleu: 2.76164\tLR: 0.00000070\t\n",
            "Steps: 11600\tLoss: 15048.29492\tPPL: 220.00578\tbleu: 2.78071\tLR: 0.00000049\t\n",
            "Steps: 11700\tLoss: 15056.10547\tPPL: 220.62256\tbleu: 2.78173\tLR: 0.00000049\t\n",
            "Steps: 11800\tLoss: 15057.82910\tPPL: 220.75896\tbleu: 2.79257\tLR: 0.00000049\t\n",
            "Steps: 11900\tLoss: 15054.41016\tPPL: 220.48859\tbleu: 2.79422\tLR: 0.00000049\t\n",
            "Steps: 12000\tLoss: 15057.29102\tPPL: 220.71631\tbleu: 2.79857\tLR: 0.00000049\t\n",
            "Steps: 12100\tLoss: 15060.35059\tPPL: 220.95851\tbleu: 2.79151\tLR: 0.00000049\t\n",
            "Steps: 12200\tLoss: 15053.90137\tPPL: 220.44830\tbleu: 2.77133\tLR: 0.00000034\t\n",
            "Steps: 12300\tLoss: 15052.71973\tPPL: 220.35498\tbleu: 2.77374\tLR: 0.00000034\t\n",
            "Steps: 12400\tLoss: 15051.43750\tPPL: 220.25372\tbleu: 2.78284\tLR: 0.00000034\t\n",
            "Steps: 12500\tLoss: 15054.01465\tPPL: 220.45724\tbleu: 2.78528\tLR: 0.00000034\t\n",
            "Steps: 12600\tLoss: 15054.42285\tPPL: 220.48953\tbleu: 3.10480\tLR: 0.00000034\t\n",
            "Steps: 12700\tLoss: 15056.51172\tPPL: 220.65465\tbleu: 2.80595\tLR: 0.00000034\t\n",
            "Steps: 12800\tLoss: 15054.48047\tPPL: 220.49416\tbleu: 3.11418\tLR: 0.00000024\t\n",
            "Steps: 12900\tLoss: 15053.40820\tPPL: 220.40941\tbleu: 3.09279\tLR: 0.00000024\t\n",
            "Steps: 13000\tLoss: 15055.24219\tPPL: 220.55429\tbleu: 3.11194\tLR: 0.00000024\t\n",
            "Steps: 13100\tLoss: 15054.07812\tPPL: 220.46230\tbleu: 3.09082\tLR: 0.00000024\t\n",
            "Steps: 13200\tLoss: 15052.52051\tPPL: 220.33922\tbleu: 2.78554\tLR: 0.00000024\t\n",
            "Steps: 13300\tLoss: 15051.49316\tPPL: 220.25812\tbleu: 2.78824\tLR: 0.00000024\t\n",
            "Steps: 13400\tLoss: 15052.67480\tPPL: 220.35141\tbleu: 2.78663\tLR: 0.00000017\t\n",
            "Steps: 13500\tLoss: 15053.61426\tPPL: 220.42560\tbleu: 2.79994\tLR: 0.00000017\t\n",
            "Steps: 13600\tLoss: 15053.24023\tPPL: 220.39607\tbleu: 2.78340\tLR: 0.00000017\t\n",
            "Steps: 13700\tLoss: 15054.60547\tPPL: 220.50403\tbleu: 2.79585\tLR: 0.00000017\t\n",
            "Steps: 13800\tLoss: 15055.79688\tPPL: 220.59816\tbleu: 2.79286\tLR: 0.00000017\t\n",
            "Steps: 13900\tLoss: 15056.26758\tPPL: 220.63541\tbleu: 2.79286\tLR: 0.00000017\t\n",
            "Steps: 14000\tLoss: 15055.83789\tPPL: 220.60143\tbleu: 2.78989\tLR: 0.00000012\t\n",
            "Steps: 14100\tLoss: 15056.88867\tPPL: 220.68454\tbleu: 2.78989\tLR: 0.00000012\t\n",
            "Steps: 14200\tLoss: 15058.60645\tPPL: 220.82043\tbleu: 2.78340\tLR: 0.00000012\t\n",
            "Steps: 14300\tLoss: 15058.13477\tPPL: 220.78305\tbleu: 2.78663\tLR: 0.00000012\t\n",
            "Steps: 14400\tLoss: 15058.97461\tPPL: 220.84959\tbleu: 2.78663\tLR: 0.00000012\t\n",
            "Steps: 14500\tLoss: 15058.18359\tPPL: 220.78696\tbleu: 2.80294\tLR: 0.00000012\t\n",
            "Steps: 14600\tLoss: 15057.65723\tPPL: 220.74525\tbleu: 2.79585\tLR: 0.00000008\t\n",
            "Steps: 14700\tLoss: 15057.19727\tPPL: 220.70894\tbleu: 2.78528\tLR: 0.00000008\t\n",
            "Steps: 14800\tLoss: 15058.27051\tPPL: 220.79379\tbleu: 2.79204\tLR: 0.00000008\t\n",
            "Steps: 14900\tLoss: 15057.38965\tPPL: 220.72409\tbleu: 2.79231\tLR: 0.00000008\t\n",
            "Steps: 15000\tLoss: 15056.93555\tPPL: 220.68822\tbleu: 2.77880\tLR: 0.00000008\t\n",
            "Steps: 15100\tLoss: 15056.68457\tPPL: 220.66833\tbleu: 2.78284\tLR: 0.00000008\t\n",
            "Steps: 15200\tLoss: 15056.33594\tPPL: 220.64076\tbleu: 2.78284\tLR: 0.00000006\t\n",
            "Steps: 15300\tLoss: 15056.89453\tPPL: 220.68495\tbleu: 2.77880\tLR: 0.00000006\t\n",
            "Steps: 15400\tLoss: 15056.64355\tPPL: 220.66507\tbleu: 2.77880\tLR: 0.00000006\t\n",
            "Steps: 15500\tLoss: 15056.87109\tPPL: 220.68317\tbleu: 2.77880\tLR: 0.00000006\t\n",
            "Steps: 15600\tLoss: 15057.30664\tPPL: 220.71759\tbleu: 2.77854\tLR: 0.00000006\t\n",
            "Steps: 15700\tLoss: 15056.51172\tPPL: 220.65465\tbleu: 2.77343\tLR: 0.00000006\t\n",
            "Steps: 15800\tLoss: 15056.37891\tPPL: 220.64423\tbleu: 2.78284\tLR: 0.00000004\t\n",
            "Steps: 15900\tLoss: 15056.37305\tPPL: 220.64371\tbleu: 2.78960\tLR: 0.00000004\t\n",
            "Steps: 16000\tLoss: 15056.39551\tPPL: 220.64551\tbleu: 2.78960\tLR: 0.00000004\t\n",
            "Steps: 16100\tLoss: 15056.22461\tPPL: 220.63203\tbleu: 2.78960\tLR: 0.00000004\t\n",
            "Steps: 16200\tLoss: 15056.50195\tPPL: 220.65390\tbleu: 2.78554\tLR: 0.00000004\t\n",
            "Steps: 16300\tLoss: 15056.71289\tPPL: 220.67064\tbleu: 2.78554\tLR: 0.00000004\t\n",
            "Steps: 16400\tLoss: 15057.13672\tPPL: 220.70410\tbleu: 2.78554\tLR: 0.00000003\t\n",
            "Steps: 16500\tLoss: 15057.49023\tPPL: 220.73210\tbleu: 2.78284\tLR: 0.00000003\t\n",
            "Steps: 16600\tLoss: 15057.78418\tPPL: 220.75536\tbleu: 2.78554\tLR: 0.00000003\t\n",
            "Steps: 16700\tLoss: 15057.88281\tPPL: 220.76315\tbleu: 2.78284\tLR: 0.00000003\t\n",
            "Steps: 16800\tLoss: 15057.92383\tPPL: 220.76640\tbleu: 2.78284\tLR: 0.00000003\t\n",
            "Steps: 16900\tLoss: 15058.24902\tPPL: 220.79210\tbleu: 2.78284\tLR: 0.00000003\t\n",
            "Steps: 17000\tLoss: 15058.30078\tPPL: 220.79620\tbleu: 2.78284\tLR: 0.00000002\t\n",
            "Steps: 17100\tLoss: 15058.36719\tPPL: 220.80147\tbleu: 2.78284\tLR: 0.00000002\t\n",
            "Steps: 17200\tLoss: 15058.53418\tPPL: 220.81474\tbleu: 2.78284\tLR: 0.00000002\t\n",
            "Steps: 17300\tLoss: 15058.47559\tPPL: 220.81010\tbleu: 2.78258\tLR: 0.00000002\t\n",
            "Steps: 17400\tLoss: 15058.54102\tPPL: 220.81526\tbleu: 2.78258\tLR: 0.00000002\t\n",
            "Steps: 17500\tLoss: 15058.62402\tPPL: 220.82181\tbleu: 2.78258\tLR: 0.00000002\t\n",
            "Steps: 17600\tLoss: 15058.66309\tPPL: 220.82495\tbleu: 2.78258\tLR: 0.00000001\t\n",
            "Steps: 17700\tLoss: 15058.78125\tPPL: 220.83423\tbleu: 2.78284\tLR: 0.00000001\t\n",
            "Steps: 17800\tLoss: 15058.78906\tPPL: 220.83485\tbleu: 2.78284\tLR: 0.00000001\t\n",
            "Steps: 17900\tLoss: 15058.82812\tPPL: 220.83800\tbleu: 2.78284\tLR: 0.00000001\t\n",
            "Steps: 18000\tLoss: 15058.70117\tPPL: 220.82790\tbleu: 2.78284\tLR: 0.00000001\t\n",
            "Steps: 18100\tLoss: 15058.59961\tPPL: 220.81990\tbleu: 2.78284\tLR: 0.00000001\t\n",
            "Steps: 18200\tLoss: 15058.66602\tPPL: 220.82516\tbleu: 2.78258\tLR: 0.00000001\t\n"
          ]
        }
      ],
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$data_path/models/${src}${tgt}_transformer/validations.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNfXOj6py_kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822bb395-d587-4c9d-81d8-b2c5a127f87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-05 21:21:27,655 - INFO - root - Hello! This is Joey-NMT (version 1.5).\n",
            "2023-05-05 21:21:28,373 - INFO - joeynmt.prediction - Loading model from models/caen_transformer/900.ckpt\n",
            "2023-05-05 21:21:32,201 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-05-05 21:21:32,415 - INFO - joeynmt.model - Enc-dec model built.\n"
          ]
        }
      ],
      "source": [
        "! cd joeynmt; python3 -m joeynmt translate \"/content/$data_path/models/${src}${tgt}_transformer/config.yaml\" < data/$FILE_VALIDATION_S > data/test.ca-en.CaTranslation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}